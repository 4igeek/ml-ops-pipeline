{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-To-End MLOps Pipeline With SageMaker (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project we will have the ability to run our functions in local mode or not. By setting this to `True` we will run all of the pipelines locally (and not connect to AWS). Setting it to `False` will run the pipeline in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load the environment variables that we need to run this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# This stops SageMaker from reporting in this cell.\n",
    "logging.getLogger('sagemaker.config').disabled = True\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "import sagemaker\n",
    "\n",
    "CODE_FOLDER = Path(\"code\") \n",
    "CODE_FOLDER.mkdir(exist_ok=True) \n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"penguins.csv\" # Path to data\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True) # Testing library\n",
    "\n",
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "COMET_API_KEY = os.environ.get(\"COMET_API_KEY\", None)\n",
    "COMET_PROJECT_NAME = os.environ.get(\"COMET_PROJECT_NAME\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise our pipeline session and initialise required config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else LocalPipelineSession(default_bucket=bucket)\n",
    "instance_type = \"ml.m5.xlarge\" if not LOCAL_MODE else \"local\"\n",
    "\n",
    "config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": instance_type,\n",
    "        \"image\": None,\n",
    "    }\n",
    "\n",
    "config[\"framework_version\"] = \"2.12\"\n",
    "config[\"py_version\"] = \"py310\"\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\"\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Data Are We Dealing With?\n",
    "There is a file in this project called [`data_analysis.ipynb`](https://github.com/4igeek/ml-ops-pipeline/blob/main/program/data_analysis.ipynb) and in that file I conduct some exploratory data analysis so that we can understand the dataset that we are going to be working with. If you're interested then make sure you take a read of that file before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting & Transforming The Data\n",
    "Now we're going to start building a pipeline (which is just a chain of components). The first component will be for transforming our data.\n",
    "\n",
    "*Note: If you haven't set up the project yet then you need to read [Configuring AWS & Local Environment for MLOps Pipelines Project](https://digitalredneck.co.uk/configuring-aws-local-environment-for-mlops-pipelines-project/) to get up to this point.* \n",
    "\n",
    "We've already uploaded our data to s3 so what this pipeline component is going to do is go to the s3 bucket and retrieve the data and then create a model out of that dataset. In order to do that there are a number of steps that we need to go through. We need to process the data, then we need to train the model, then we need to evaluate the model to make sure it is producing good results, and then we'll be in the position where we can register the model that we can then deploy for use in the future.\n",
    "\n",
    "The pipeline is the structure that is going to combine all of these steps and execute them one after the other.\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/PipeLine_split_transform.jpg\" style=\"width: 100%; height: auto;\" />\n",
    "\n",
    "This step is going to take the data from our s3 bucket and then split the data (training and testing sets) and transform the data (replacing missing values etc). The output of this component is going to be saved back in s3. Then the next step will be training and then evaluation etc.\n",
    "\n",
    "We'll use the [Scikit-Learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for the transformations, and a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) to execute a processing script. For more information on this check out the [SageMaker Pipelines Overview](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) for an intro into the fundamental components of a SageMaker Pipeline.\n",
    "\n",
    "## Step 1: The Processing Script\n",
    "We need to write a script that will be used to split and transform the data. This Processing Step will create a SageMaker Processing Job in the background. It will then run the script and upload the output to AWS s3.\n",
    "\n",
    "We will create a folder called \"processing\" and add it to the system path so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line in the following cell i.e. `%%writefile {CODE_FOLDER}/processing/script.py` essentially writes the code written in the cell to a new file in a folder called `processing` without executing it. This means we're using Jupyter to write python files that will then be used in production (and not Jupyter itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/processing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load, split, and transform the data.\"\"\"\n",
    "\n",
    "    # Get all of the data in our data directory and receive a \n",
    "    # shuffled dataset that contains all of the data.\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    # This is where we create a SciKit-Learn pipeline to transform the dataset.\n",
    "\n",
    "    # Create a ColumnTransformer for the target feature.\n",
    "    # This will transform the categorical data in the target feature\n",
    "    # using an ordinal encoder: so 0, 1, 2 etc for the different classes.\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])],\n",
    "    )\n",
    "\n",
    "    # This is applied to all of the numerical columns in the dataset.\n",
    "    # We're going to impute missing values with the mean of all the other values.\n",
    "    # We're also going to scale (normalise) all of the values.\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "    # This will be applied to all of the categorical columns in the dataset.\n",
    "    # We're going to impute missing values with the most common of all the other values.\n",
    "    # We're also going to one-hot encode the columns to create new features.\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(),\n",
    "    )\n",
    "\n",
    "    # This is where we use the transformers we just created in a ColumnTransformer.\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\", # The name of this transformer\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\n",
    "                \"categorical\", # The name of this transformer \n",
    "                categorical_transformer, \n",
    "                [\"island\"]), # Only being applied to the island column.\n",
    "                # We don't transform the \"sex\" column and as such SciKit Learn \n",
    "                # will drop it. By ignoring it it will be dropped.\n",
    "                # The reason we're doing that is because \"sex\" doesn't have any \n",
    "                # predictive power in this dataset and as such we don't need it.\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Now that the data has been transformed we're now going to split it.\n",
    "    # We need to split the data before transforming it.\n",
    "    # 70% for training, 15% for validation, and 15% for test\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    # This is going to help us when we get to monitoring our model\n",
    "    # These are the baselines for our raw data.\n",
    "    _save_train_baseline(base_directory, df_train)\n",
    "    _save_test_baseline(base_directory, df_test)\n",
    "\n",
    "    # This is where we now transform the data. This is how the transformers are applied.\n",
    "    # This will be applied to all of the sets in our data i.e. train, validation, and test.\n",
    "    # We \"fit\" the train set and from the info collected from that we will then transform\n",
    "    # The validation and test set using the info collected by calling \"fit_transform\".\n",
    "    \n",
    "    # The reason we do this is in case some of the classes are missing in the validation \n",
    "    # or test sets. If they are we will know because we got that from \"fit_transform\"\n",
    "    # By doing it this way, we can ensure that the encodings of the data are the same \n",
    "    # in all sets i.e. class A=0, class B=1, class C=2 etc. \n",
    "    y_train = target_transformer.fit_transform( # fit_transform \n",
    "        np.array(df_train.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_validation = target_transformer.transform( # transform\n",
    "        np.array(df_validation.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_test = target_transformer.transform( # transform\n",
    "        np.array(df_test.species.values).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "    # Now that we've got our \"y\" labels we can drop them from the three main datasets.\n",
    "    # We are going to drop the target feature for the three sets.\n",
    "    df_train = df_train.drop(\"species\", axis=1)\n",
    "    df_validation = df_validation.drop(\"species\", axis=1)\n",
    "    df_test = df_test.drop(\"species\", axis=1)\n",
    "\n",
    "    # This is where we transform the features in the dataset (minus the target column).\n",
    "    X_train = features_transformer.fit_transform(df_train)  # noqa: N806\n",
    "    X_validation = features_transformer.transform(df_validation)  # noqa: N806\n",
    "    X_test = features_transformer.transform(df_test)  # noqa: N806\n",
    "\n",
    "    # Once this is done our data is ready to be saved.\n",
    "    _save_splits(\n",
    "        base_directory,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "\n",
    "    # This is where we save the SciKit Learn transformation pipeline.\n",
    "    # When we deploy this model in production we are going to be receiving raw data.\n",
    "    # Before we can make a prediction on that raw data we need to transform the data\n",
    "    # so that it works with the model (i.e. has had the same transformations made to it).\n",
    "    # We need to transform that raw data using the exact same process as we have here.\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # This script will grab data out of every csv file in the directory\n",
    "    # This is because more data can be added to the project (in separate files)\n",
    "    # And that new data will be included in the processing step.\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    # If there are no files then we are going to raise an error\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    # If there are files then we are going to save them to a DataFrame\n",
    "    # This DataFrame will contain the data from all of the files in the directory.\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle and return the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df):\n",
    "    \"\"\"Split the data into train, validation, and test.\"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_train_baseline(base_directory, df_train):\n",
    "    \"\"\"Save the untransformed training data to disk.\n",
    "\n",
    "    We will need the training data to compute a baseline to\n",
    "    determine the quality of the data that the model receives\n",
    "    when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"train-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_train.copy().dropna()\n",
    "\n",
    "    # To compute the data quality baseline, we don't need the\n",
    "    # target variable, so we'll drop it from the dataframe.\n",
    "    df = df.drop(\"species\", axis=1)\n",
    "\n",
    "    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "def _save_test_baseline(base_directory, df_test):\n",
    "    \"\"\"Save the untransformed test data to disk.\n",
    "\n",
    "    We will need the test data to compute a baseline to\n",
    "    determine the quality of the model predictions when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"test-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_test.copy().dropna()\n",
    "\n",
    "    # We'll use the test baseline to generate predictions later,\n",
    "    # and we can't have a header line because the model won't be\n",
    "    # able to make a prediction for it.\n",
    "    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_validation,  # noqa: N803\n",
    "    y_validation,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory, target_transformer, features_transformer):\n",
    "    \"\"\"Save the Scikit-Learn transformation pipelines.\n",
    "\n",
    "    This function creates a model.tar.gz file that\n",
    "    contains the two transformation pipelines we built\n",
    "    to transform the data.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n",
    "        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n",
    "            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                Path(directory) / \"features.joblib\", arcname=\"features.joblib\",\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run some unit tests on the script to ensure everything worked okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.37s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "# | code-fold: true\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import pytest\n",
    "from processing.script import preprocess\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(base_directory=directory)\n",
    "\n",
    "    yield directory\n",
    "\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generates_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generates_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def test_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # After transforming the data, the number of features should be 7:\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm = 1\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    number_of_features = 7\n",
    "\n",
    "    # The transformed splits should have an additional column for the target\n",
    "    # variable.\n",
    "    assert train.shape[1] == number_of_features + 1\n",
    "    assert validation.shape[1] == number_of_features + 1\n",
    "    assert test.shape[1] == number_of_features + 1\n",
    "\n",
    "\n",
    "def test_train_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"train-baseline\" / \"train-baseline.csv\",\n",
    "        header=None,\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 0].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_test_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"test-baseline\" / \"test-baseline.csv\", header=None\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 1].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_train_baseline_includes_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n",
    "    assert baseline.columns[0] == \"island\"\n",
    "\n",
    "\n",
    "def test_test_baseline_does_not_include_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n",
    "    assert baseline.columns[0] != \"island\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Caching Config\n",
    "SageMaker supports caching which means it will try to execute a previous run of the same step (if nothing has changed). For more info check out [Caching Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html).\n",
    "\n",
    "Let's define a caching policy that we can use at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pipeline Config\n",
    "We can make our pipeline more flexible by parameterising it. In our case we are going to pass through the location of the dataset. This means we can then switch out datasets by changing the value of this parameter. For more information visit [Pipeline Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html).\n",
    "\n",
    "Imagine you had a massive dataset and you wanted to test something out; you wouldn't necessarily want to run this test on the entire dataset and rather just a sub-set of the data. This is how you would do it by simply pointing the pipeline to the smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\", # Name of the ParameterString\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setting up the Processing Step\n",
    "\n",
    "*Note: New AWS accounts don't automatically have access to `ml.m5.xlarge` so you need to make sure you've applied for this. You can read more by visiting [Configuring AWS & Local Environment for MLOps Pipelines Project](https://digitalredneck.co.uk/configuring-aws-local-environment-for-mlops-pipelines-project/) and ensuring you're all properly set up.*\n",
    "\n",
    "We're now going to set up a processing step. We need this for when we run the pipeline because when we do, SageMaker (internally) will create a cluster and it will run multiple instances (in our case only one) and it will deploy a processing container in the cloud.\n",
    "\n",
    "SageMaker will copy the data from s3 into the processing container, it will then run the script that we created and when it finished it will grab anything in the output directory and then save it back to s3.\n",
    "\n",
    "Seeing as we're using SciKit-Learn, we need to tell SageMaker to run the script in a container that supports SciKit-Learn. The way we do this is by either specifying all of the criteria you need or we can make use of the internal classes in SageMaker to handle this for us (we'll be using the SciKit-Learn processor class). You need a processor to define a processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\", # Defines the job name that we will see in SageMaker.\n",
    "    framework_version=\"1.2-1\", # We need to tell SageMaker which version of SciKit-Learn we're using.\n",
    "    instance_type=config[\"instance_type\"], # What type of computer do I want to use.\n",
    "    instance_count=1, # The number of instances in the cluster generated by SageMaker\n",
    "    role=role, # What is the role that the job is going to be running under (environment variable)\n",
    "    sagemaker_session=config[\"session\"], # This is the session we're going to be running under.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the Processing Step that we'll use in our pipeline. This is the step that we add to the pipeline.\n",
    "\n",
    "We specify the list of inputs. In this instance it is the dataset that we stored in s3 (make sure you've completed that by visiting [Configuring AWS & Local Environment for MLOps Pipelines Project](https://digitalredneck.co.uk/configuring-aws-local-environment-for-mlops-pipelines-project/) to ensure you're properly set up to run this notebook project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\", # We can see this in SageMaker\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(CODE_FOLDER / 'processing' / 'script.py').as_posix()}\", # What code does the processing job need to run?\n",
    "        inputs=[ # This is where we tell the processing step where the dataset is i.e. the inputs.\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\", # SageMaker will auto download the data into this location.\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[ # Now we specify the outputs\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\", # Points to the directory inside the processing container.\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\", # Optional as SageMaker will save automatically.\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/validation\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"model\",\n",
    "                source=\"/opt/ml/processing/model\", # We're saving our processing pipeline to a model.tar.gz file.\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/model\", # This is where we save it to.\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train-baseline\",\n",
    "                source=\"/opt/ml/processing/train-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test-baseline\",\n",
    "                source=\"/opt/ml/processing/test-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config, # We make sure the step is cached.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Creating the pipeline\n",
    "We can no create a pipeline and submits its definition to the SageMaker Pipeline Service to create the pipeline if it doesn't exist (or update it if it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import submit_pipeline\n",
    "\n",
    "steps = [preprocessing_step] # type: ignore\n",
    "pipeline_name = \"data-transform-pipeline\"\n",
    "\n",
    "pipeline = submit_pipeline(dataset_location, pipeline_name, steps, pipeline_definition_config, config, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# | eval: false\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided you ran the above cell NOT in local mode you can now see this pipeline in SageMaker studio under the tab pipelines.\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/SageMaker_pipelines.jpg\" style=\"width:100%; height: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully created our first step in the pipeline. You'll notice a trend moving forwards as we begin to create the next steps in the project. In the next phase we're going to define a training process (and other steps) and you're going to find that we'll be creating scripts, we'll be defining the container that we want that script to run on, we'll be adding those new steps to the pipeline that we just create and everything will become familiar to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "Now we're going to extend the pipeline that we just created with a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training). We're going to be using TensorFlow for that so if you need to know more about that then visit the [TensorFlow docs in SageMaker](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#train-a-model-with-tensorflow).\n",
    "\n",
    "This will work by taking the output from the last step (the processing step) and then using that as in input into this step where we'll train a machine learning model. This step will then output a model that we will then evaluate it in the next step.\n",
    "\n",
    "We're going to be working on this next:\n",
    "<img src=\"https://digitalredneck.co.uk/SageMaker_training_step.jpg\" style=\"width:100%; height:auto;\"/>\n",
    "\n",
    "*Note: Notice how we are using the data from the processing job (\"Dataset splits\" in the image above).*\n",
    "\n",
    "We're also going to implement tracking using [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) and [Comet ML](https://www.comet.com/site/).\n",
    "\n",
    "To do this we're going to create a new folder in the project called `training` and this will house the script (which we can then use later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"training\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/training/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "from pathlib import Path\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import Input\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from packaging import version\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(\n",
    "    model_directory,\n",
    "    train_path,\n",
    "    validation_path,\n",
    "    pipeline_path,\n",
    "    experiment,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "):\n",
    "    print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "    # We're now loading the data in memory and splitting the labels\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation = X_validation.drop(X_validation.columns[-1], axis=1)\n",
    "\n",
    "    # We're going to be building a simple sequential model using Keras.\n",
    "    # The values in the output layer will add up to 1 and the softmax\n",
    "    # Function is going to return the one with the highest value.\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(X_train.shape[1],)),\n",
    "            Dense(10, activation=\"relu\"),\n",
    "            Dense(8, activation=\"relu\"),\n",
    "            Dense(3, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Now we compile the model using a SGD and categorical crossentropy\n",
    "    # and the metrics we will be tracking is the accuracy of the model.\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Then we fit the model\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_validation, y_validation), # We pass in the validation set\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2, # What type of logging do we need (0, 1, or 2)\n",
    "    )\n",
    "\n",
    "    # For logging purposes we're logging out the validation accuracy\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    # This is a SciKit-Learn function that retrieves our accuracy\n",
    "    val_accuracy = accuracy_score(y_validation, predictions)\n",
    "    print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Starting on version 3, Keras changed the model saving format.\n",
    "    # Since we are running the training script using two different versions\n",
    "    # of Keras, we need to check to see which version we are using and save\n",
    "    # the model accordingly.\n",
    "    model_filepath = (\n",
    "        Path(model_directory) / \"001\" # Numerical directory indicates the model version (for TensorFlow)\n",
    "        if version.parse(keras.__version__) < version.parse(\"3\")\n",
    "        else Path(model_directory) / \"penguins.keras\"\n",
    "    )\n",
    "\n",
    "    # Then we save the model\n",
    "    model.save(model_filepath)\n",
    "\n",
    "    # This model expects transformed data and as such we need to run the following \n",
    "    # as we need to save this info (transformation pipeline) along with the model.\n",
    "    # Let's save the transformation pipelines inside the\n",
    "    # model directory so they get bundled together.\n",
    "    with tarfile.open(Path(pipeline_path) / \"model.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(model_directory)\n",
    "\n",
    "    # If we have a Comet experiment running then we need to update it with the two \n",
    "    # parameters i.e. epochs and batch_size.\n",
    "    if experiment:\n",
    "        experiment.log_parameters(\n",
    "            {\n",
    "                \"epochs\": epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        )\n",
    "        experiment.log_dataset_hash(X_train)\n",
    "\n",
    "        # Create a confusion matrix and it is going to log it\n",
    "        experiment.log_confusion_matrix(\n",
    "            y_validation.astype(int), predictions.astype(int)\n",
    "        )\n",
    "\n",
    "        # Log the model (the output of this process)\n",
    "        experiment.log_model(\"penguins\", model_filepath.as_posix())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to\n",
    "    # the entry point as script arguments.\n",
    "    # SageMaker is going to use the command line to call this script\n",
    "    # and it is going to parse arguments to this script.\n",
    "    # For now we want to know the number of epochs and the batch size for the model.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Let's create a Comet experiment to log the metrics and parameters\n",
    "    # of this training job.\n",
    "    # We are going to be using Comet ML to keep track of our experiments.\n",
    "    comet_api_key = os.environ.get(\"COMET_API_KEY\", None)\n",
    "    comet_project_name = os.environ.get(\"COMET_PROJECT_NAME\", None)\n",
    "\n",
    "    # If the Comet ML API key and project name are provided then we will create the experiment.\n",
    "    experiment = (\n",
    "        Experiment(\n",
    "            project_name=comet_project_name,\n",
    "            api_key=comet_api_key,\n",
    "            auto_metric_logging=True,\n",
    "            auto_param_logging=True,\n",
    "            log_code=True,\n",
    "        )\n",
    "        if comet_api_key and comet_project_name\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # SageMaker will automatically create an environment variable called \"SM_TRAINING_ENV\" and\n",
    "    # we're reading in that variable. Inside that variable (which is a JSON object) there is \n",
    "    # a value called \"job_name\" which we are also reading in.\n",
    "    training_env = json.loads(os.environ.get(\"SM_TRAINING_ENV\", {}))\n",
    "    job_name = training_env.get(\"job_name\", None) if training_env else None\n",
    "\n",
    "    # We want to use the SageMaker's training job name as the name\n",
    "    # of the experiment so we can easily recognise it.\n",
    "    if job_name and experiment:\n",
    "        experiment.set_name(job_name)\n",
    "\n",
    "    train(\n",
    "        # This is the location where we need to save our model.\n",
    "        # SageMaker will create a model.tar.gz file with anything\n",
    "        # inside this directory when the training script finishes.\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        # SageMaker creates one channel for each one of the inputs\n",
    "        # to the Training Step.\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n",
    "        pipeline_path=os.environ[\"SM_CHANNEL_PIPELINE\"],\n",
    "        experiment=experiment,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to run some unit tests before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.14.0\n",
      "8/8 - 0s - loss: 1.2113 - accuracy: 0.2050 - val_loss: 1.1565 - val_accuracy: 0.2353 - 459ms/epoch - 57ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Validation accuracy: 0.23529411764705882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\conta\\AppData\\Local\\Temp\\tmpmmyu1qal\\model\\001\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0mKeras version: 2.14.0\n",
      "8/8 - 0s - loss: 1.0050 - accuracy: 0.4979 - val_loss: 1.0572 - val_accuracy: 0.4510 - 496ms/epoch - 62ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Validation accuracy: 0.45098039215686275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\conta\\AppData\\Local\\Temp\\tmpono97i3h\\model\\001\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.19s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "#| code-fold: true\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pytest\n",
    "import tempfile\n",
    "\n",
    "from processing.script import preprocess\n",
    "from training.script import train\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    preprocess(base_directory=directory)\n",
    "    train(\n",
    "        model_directory=directory / \"model\",\n",
    "        train_path=directory / \"train\", \n",
    "        validation_path=directory / \"validation\",\n",
    "        pipeline_path=directory / \"model\",\n",
    "        experiment=None,\n",
    "        epochs=1 \n",
    "    )\n",
    "    \n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_train_bundles_model_assets(directory):\n",
    "    bundle = os.listdir(directory / \"model\")\n",
    "    assert \"001\" in bundle\n",
    "    \n",
    "    assets = os.listdir(directory / \"model\" / \"001\")\n",
    "    assert \"saved_model.pb\" in assets\n",
    "\n",
    "\n",
    "def test_train_bundles_transformation_pipelines(directory):\n",
    "    bundle = os.listdir(directory / \"model\")\n",
    "    assert \"target.joblib\" in bundle\n",
    "    assert \"features.joblib\" in bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating the Training Step\n",
    "Now we can create the [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training). Before we do that however, considering that SageMaker is going to run this for us, we need to tell SageMaker that we're using Comet ML. That's what the next cell is achieving. We create this `requirements.txt` file inside the same folder that we run the scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/training/requirements.txt\n",
    "#| label: requirements.txt\n",
    "#| filename: requirements.txt\n",
    "#| code-line-numbers: false\n",
    "\n",
    "comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we created the data processing step, we used a processor but when creating a training step we will use something called an estimator. SageMaker uses this estimator to handle end-to-end training and deployment tasks. In the example below we're going to be using the [TensorFlow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.tensorflow.html#tensorflow-estimator) to run our script.\n",
    "\n",
    "You can read more about this by visiting [SageMaker Estimators](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    base_job_name=\"training\",\n",
    "    entry_point=\"script.py\",\n",
    "    source_dir=f\"{(CODE_FOLDER / 'training').as_posix()}\",\n",
    "    \n",
    "    # SageMaker will pass these hyperparameters as arguments\n",
    "    # to the entry point of the training script.\n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": 32,\n",
    "    },\n",
    "    \n",
    "    # SageMaker will create these environment variables on the\n",
    "    # Training Job instance.\n",
    "    environment={\n",
    "        \"COMET_API_KEY\": COMET_API_KEY,\n",
    "        \"COMET_PROJECT_NAME\": COMET_PROJECT_NAME,\n",
    "    },\n",
    "    \n",
    "    # SageMaker will track these metrics as part of the experiment\n",
    "    # associated to this pipeline. The metric definitions tells\n",
    "    # SageMaker how to parse the values from the Training Job logs.\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "    ],\n",
    "    image_uri=config[\"image\"],\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the Training Step. This will create a SageMaker Training Job in the background, run our script, and then upload the output of that to our s3 bucket.\n",
    "\n",
    "This step will take the train and validation data from the preprocessing step that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "def create_training_step(estimator):\n",
    "    \"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\n",
    "    return TrainingStep(\n",
    "        name=\"train-model\",\n",
    "        step_args=estimator.fit(\n",
    "            inputs={\n",
    "                \"train\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[ # type: ignore\n",
    "                        \"train\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"validation\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[ # type: ignore\n",
    "                        \"validation\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"pipeline\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[ # type: ignore\n",
    "                        \"model\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"application/tar+gzip\",\n",
    "                ),\n",
    "            },\n",
    "        ),\n",
    "        cache_config=cache_config,\n",
    "    )\n",
    "\n",
    "\n",
    "train_model_step = create_training_step(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Creating the pipeline\n",
    "Now we need to define the SageMaker Pipeline and submit the definition to the SageMaker Pipelines service. This will create the pipeline if it doesn't exist, to update it if it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from functions import submit_pipeline\n",
    "\n",
    "steps = [\n",
    "            preprocessing_step, # type: ignore\n",
    "            train_model_step # type: ignore\n",
    "        ] \n",
    "pipeline_name = \"training-pipeline\"\n",
    "\n",
    "pipeline = submit_pipeline(dataset_location, pipeline_name, steps, pipeline_definition_config, config, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the pipeline as we did before for the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# | eval: false\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have just created the 2nd step in our pipeline. You can run this NOT in local mode to push this step to SageMaker Studio and once there you can look for a pipeline called `\"training-pipeline\"`. Once you have located that and have clicked into it you'll see an execution of that step. If you click into that you'll see a map with our two steps.\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/SageMaker_model.jpg\" style=\"width: 100%; height: auto;\"/>\n",
    "\n",
    "*Note: The order in which we send the steps through to SageMaker doesn't matter as SageMaker will determine the order in which they run.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also, at this point, log into Comet ML and take a look at the results from our training.\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/CometML_experiments.jpg\" style=\"width: 100%; height: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Container\n",
    "Now we're going to create a custom Docker image to train our model. This will give us full control over the environment where the scripts will be executed.\n",
    "\n",
    "## Step 1: Preparing the Docker Image\n",
    "The first thing that we need to do is copy the training script a folder where we'll prepare the Docker image. We're going to use the same script that we created before for this because it is compatible with the latest version of Keras.\n",
    "\n",
    "*Note: We're going to be running the script using [Keras 3](https://keras.io/) with a [JAX](https://jax.readthedocs.io/en/latest/index.html) backend.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('code/containers/training/train.py')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "(CODE_FOLDER / \"containers\" / \"training\").mkdir(parents=True, exist_ok=True)\n",
    "shutil.copy2(\n",
    "    CODE_FOLDER / \"training\" / \"script.py\",\n",
    "    CODE_FOLDER / \"containers\" / \"training\" / \"train.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to install the libraries needed in the training container. Just like we did before we'll create a `requirements.txt` file that SageMaker will execute before the container is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/containers/training/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/containers/training/requirements.txt\n",
    "# | filename: requirements.txt\n",
    "# | code-line-numbers: true\n",
    "\n",
    "sagemaker-training\n",
    "packaging\n",
    "keras\n",
    "pandas\n",
    "scikit-learn\n",
    "comet_ml\n",
    "jax[cpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the Dockerfile which will contain the instructions needed to build the training image. This will automatically run the `train.py` script once it starts.\n",
    "\n",
    "*Note: To use JAX backend we need to set the `KERAS_BACKEND` environment variable to `jax`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/containers/training/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/containers/training/Dockerfile\n",
    "# | filename: Dockerfile\n",
    "# | code-line-numbers: true\n",
    "\n",
    "FROM python:3.10-slim\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "    python3 \\\n",
    "    build-essential libssl-dev pkg-config libhdf5-dev\n",
    "\n",
    "# Let's install the required Python packages from \n",
    "# the requirements.txt file.\n",
    "COPY requirements.txt .\n",
    "RUN pip install --user --upgrade pip\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "# We are going to be running the training script\n",
    "# as the entrypoint of this container.\n",
    "COPY train.py /opt/ml/code/train.py\n",
    "ENV SAGEMAKER_PROGRAM = train.py\n",
    "\n",
    "# We want to use JAX as the backend for Keras.\n",
    "ENV KERAS_BACKEND = jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the dockerfile we can now build the image using the `docker build` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "IMAGE_NAME = \"keras-custom-training-container\"\n",
    "\n",
    "if LOCAL_MODE:\n",
    "    # If we are running in Local Mode, we can use the\n",
    "    # default Docker build command.\n",
    "    print(\"Building Docker image for arm64 architecture...\")\n",
    "\n",
    "    !docker build -t $IMAGE_NAME $CODE_FOLDER/containers/training/\n",
    "else:\n",
    "    # If we aren't running the code in Local Mode, we need\n",
    "    # to specify we want to build the Docker image for the\n",
    "    # linux/amd64 architecture before uploading it to ECR.\n",
    "    print(\"Building Docker image for linux/amd64 architecture...\")\n",
    "\n",
    "    !docker build --platform=\"linux/amd64\" -t $IMAGE_NAME $CODE_FOLDER/containers/training/\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Publishing Image To Elastic Container Registry (ECR)\n",
    "Now that we have our image, we can push it to the ECR which is a fully managed Docker container registry where we can manage our containers. We need to perform this step to make the image that we just created available to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "account = !aws sts get-caller-identity --query Account --output text\n",
    "region = !aws configure get region\n",
    "algorithm_name =IMAGE_NAME\n",
    "\n",
    "# Extract the actual values from the captured output\n",
    "account = account[0].strip()\n",
    "region = region[0].strip()\n",
    "repository = f\"{account}.dkr.ecr.{region}.amazonaws.com/{algorithm_name}:latest\"\n",
    "check_image_command = f\"aws ecr describe-repositories --repository-names {IMAGE_NAME}\"\n",
    "\n",
    "if not LOCAL_MODE:\n",
    "    # Check to see if the repository exists already or not\n",
    "    check_image_exists = !{check_image_command}\n",
    "    if not check_image_exists[0]:\n",
    "        # The repository doesn't exist\n",
    "        !aws ecr create-repository --repository-name keras-custom-training-container\n",
    "    else:\n",
    "        # The repository does exist\n",
    "        !aws ecr describe-repositories --repository-names keras-custom-training-container\n",
    "\n",
    "    login_command = f\"aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {repository}\"\n",
    "    !{login_command}\n",
    "\n",
    "    # Use f-strings to construct and execute the bash commands\n",
    "    tag_command = f\"docker tag {algorithm_name} {repository}\"\n",
    "    !{tag_command}\n",
    "\n",
    "    push_command = f\"docker push {repository}\"\n",
    "    !{push_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setting up the Training Step\n",
    "Now let's get the name of our training image. If we're running in local mode then we'll use the name `IMAGE_NAME` but if not then we'll need to get the name of the image that we pushed to ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "tag = \":latest\"\n",
    "\n",
    "uri_suffix = \"amazonaws.com\"\n",
    "if region in [\"cn-north-1\", \"cn-northwest-1\"]:\n",
    "    uri_suffix = \"amazonaws.com.cn\"\n",
    "\n",
    "training_container_image = (\n",
    "    IMAGE_NAME\n",
    "    if LOCAL_MODE\n",
    "    else (f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{IMAGE_NAME}:latest\")\n",
    ")\n",
    "\n",
    "training_container_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) and [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training) using the function we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "keras_estimator = Estimator(\n",
    "    image_uri=training_container_image,\n",
    "    instance_count=1,\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "keras_train_model_step = create_training_step(keras_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Creating the Pipeline\n",
    "Now that we've got our training model step all configured with a custom Docker image, let's define the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import submit_pipeline\n",
    "\n",
    "steps = [\n",
    "            preprocessing_step, # type: ignore\n",
    "            keras_train_model_step # type: ignore\n",
    "        ] \n",
    "pipeline_name = \"keras-train-model-step\"\n",
    "\n",
    "pipeline = submit_pipeline(dataset_location, pipeline_name, steps, pipeline_definition_config, config, role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit the new definition to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%script false --no-raise-error\n",
    "# | eval: false\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning our Model\n",
    "This section is going to extend the current SageMaker Pipeline with a step to tune the model. This will use a Hyperparameter Tuning Job.\n",
    "\n",
    "The problem with the current pipeline is that we don't know exactly what the source data will look like. We know how to write a good model for the current problem i.e. the penguins species classification problem but we don't know whether the pipeline will create the best possible model for us. \n",
    "\n",
    "To fix this issue, can use a \"tuning step\" instead of a \"training step\" which will be automatically optimised for us (in terms of the models hyperparameters). The tuning step will train several versions of our model and then select the best one for us to use depending on its findings.\n",
    "\n",
    "That means we're going to be improving this step (that we worked on earlier)\n",
    "<img src=\"https://digitalredneck.co.uk/SageMaker_training_step.jpg\" style=\"width:100%; height:auto; margin-top:15px;\"/>\n",
    "\n",
    "*Note: This step will not work in LOCAL_MODE so we will configure it to only work if we want to be working in the cloud.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TUNING_STEP = not LOCAL_MODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Tuning Step\n",
    "The Tuning Step requires a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) to configure the tuning job.\n",
    "\n",
    "Here are some of the configuration options:\n",
    "1. `objective_metric_name`: The name of the metric tuner\n",
    "2. `objective_type`: This is the object ive of the tuner. Do we want to minimise or maximise it depending on whether we're measuring loss or accuracy.\n",
    "3. `metric_definitions`: Defines how the tuner will determine the metrics value by looking at the output logs.\n",
    "4. `max_jobs`: Set the total number of training jobs.\n",
    "5. `max_parallel_jobs`: Set the maximum number of parallel jobs to run for the tuning step.\n",
    "\n",
    "We can explore the hyperparameters using a Parameter class (seen below in relation to \"epochs\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.parameter import IntegerParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name=\"val_accuracy\",\n",
    "    objective_type=\"Maximize\",\n",
    "    hyperparameter_ranges={\n",
    "        \"epochs\": IntegerParameter(10, 50),\n",
    "    },\n",
    "    metric_definitions=[{\"Name\": \"val_accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"}],\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done that we can create the Tuning Step. When we deploy this, SageMaker will create a Hyperparameter Tuning Job that will run in the background and use the training script (that we used before) to train different versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sagemaker.workflow.steps import TuningStep\n",
    "\n",
    "tune_model_step = TuningStep(\n",
    "    name=\"tune-model\",\n",
    "    step_args=tuner.fit(\n",
    "        inputs={\n",
    "            \"train\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"train\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"validation\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"validation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"text/csv\",\n",
    "            ),\n",
    "            \"pipeline\": TrainingInput(\n",
    "                s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"model\"\n",
    "                ].S3Output.S3Uri,\n",
    "                content_type=\"application/tar+gzip\",\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating the Pipeline\n",
    "Now all we need to do is to define the SageMaker Pipeline and then submit its definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from functions import submit_pipeline\n",
    "\n",
    "steps = [\n",
    "            preprocessing_step, # type: ignore\n",
    "            tune_model_step, # type: ignore\n",
    "        ] \n",
    "pipeline_name = \"tuning-step\"\n",
    "\n",
    "pipeline = submit_pipeline(dataset_location, pipeline_name, steps, pipeline_definition_config, config, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:586242780547:pipeline/tuning-step/execution/754mo8yue885', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000001C1ADD22BF0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# | eval: false\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model\n",
    "This section extends the SageMaker Pipeline with a step that will evaluate our model for us (using a holdout set) that we created in the preprocessing step.\n",
    "\n",
    "As a reminder, this is what we're working on now.\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/SageMaker_evaluation.jpg\" style=\"width: 100%; height: auto;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this step has two inputs. It has the model and the test set that we created in the preprocessing step. We're going to use the metrics gained from this step to eventually register our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Evaluation Script\n",
    "We're going to use a [Processing Step](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.steps.ProcessingStep) to execute the evaluation script which is responsible for loading the model and evaluating it on the test set. We're going to store this script in a folder called `evaluation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"evaluation\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the script for that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/evaluation/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/evaluation/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import json\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def evaluate(model_path, test_path, output_path):\n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "    # Let's now extract the model package so we can load\n",
    "    # it in memory.\n",
    "    with tarfile.open(Path(model_path) / \"model.tar.gz\") as tar:\n",
    "        tar.extractall(path=Path(model_path))\n",
    "\n",
    "    model = keras.models.load_model(Path(model_path) / \"001\")\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "    # Let's create an evaluation report using the model accuracy.\n",
    "    evaluation_report = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\"value\": accuracy},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    with open(Path(output_path) / \"evaluation.json\", \"w\") as f:\n",
    "        f.write(json.dumps(evaluation_report))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate(\n",
    "        model_path=\"/opt/ml/processing/model/\",\n",
    "        test_path=\"/opt/ml/processing/test/\",\n",
    "        output_path=\"/opt/ml/processing/evaluation/\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to run some unit tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.14.0\n",
      "8/8 - 1s - loss: 1.0353 - accuracy: 0.5941 - val_loss: 1.0509 - val_accuracy: 0.6275 - 791ms/epoch - 99ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "Validation accuracy: 0.6274509803921569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\conta\\AppData\\Local\\Temp\\tmped8u7oco\\model\\001\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "Test accuracy: 0.6666666666666666\n",
      "\u001b[32m.\u001b[0mKeras version: 2.14.0\n",
      "8/8 - 0s - loss: 1.1661 - accuracy: 0.5983 - val_loss: 1.1440 - val_accuracy: 0.5490 - 485ms/epoch - 61ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Validation accuracy: 0.5490196078431373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\conta\\AppData\\Local\\Temp\\tmp8zval3_s\\model\\001\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "Test accuracy: 0.6862745098039216\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 4.20s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "# | code-fold: true\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import pytest\n",
    "import tempfile\n",
    "\n",
    "from processing.script import preprocess\n",
    "from training.script import train\n",
    "from evaluation.script import evaluate\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "\n",
    "    preprocess(base_directory=directory)\n",
    "\n",
    "    train(\n",
    "        model_directory=directory / \"model\",\n",
    "        train_path=directory / \"train\",\n",
    "        validation_path=directory / \"validation\",\n",
    "        pipeline_path=directory / \"model\",\n",
    "        experiment=None,\n",
    "        epochs=1,\n",
    "    )\n",
    "\n",
    "    # After training a model, we need to prepare a package just like\n",
    "    # SageMaker would. This package is what the evaluation script is\n",
    "    # expecting as an input.\n",
    "    with tarfile.open(directory / \"model.tar.gz\", \"w:gz\") as tar:\n",
    "        tar.add(directory / \"model\" / \"001\", arcname=\"001\")\n",
    "\n",
    "    evaluate(\n",
    "        model_path=directory,\n",
    "        test_path=directory / \"test\",\n",
    "        output_path=directory / \"evaluation\",\n",
    "    )\n",
    "\n",
    "    yield directory / \"evaluation\"\n",
    "\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_evaluate_generates_evaluation_report(directory):\n",
    "    output = os.listdir(directory)\n",
    "    assert \"evaluation.json\" in output\n",
    "\n",
    "\n",
    "def test_evaluation_report_contains_accuracy(directory):\n",
    "    with open(directory / \"evaluation.json\", \"r\") as file:\n",
    "        report = json.load(file)\n",
    "\n",
    "    assert \"metrics\" in report\n",
    "    assert \"accuracy\" in report[\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Referencing the Models Assets\n",
    "\n",
    "One of the inputs to the evaluation step is the model coming from either the training or tuning step. We can use a flag to determine whether to use the tuning or training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_assets = train_model_step.properties.ModelArtifacts.S3ModelArtifacts # type: ignore\n",
    "\n",
    "if USE_TUNING_STEP:\n",
    "    model_assets = tune_model_step.get_top_model_s3_uri( # type: ignore\n",
    "        top_k=0,\n",
    "        s3_bucket=config[\"session\"].default_bucket(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mapping the Output Property File\n",
    "\n",
    "We're now going to build a property file and map the evaluation report ot a property file. \n",
    "\n",
    "To learn more on this you can read [How to Build and Manage a Property File](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-propertyfile.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"evaluation-report\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Setting up the Evaluation Step\n",
    "\n",
    "To run the evaluation script we'll use a Processing Step configured with a [TensorflowProcessor](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job-frameworks-tensorflow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "\n",
    "evaluation_processor = TensorFlowProcessor(\n",
    "    base_job_name=\"evaluation-processor\",\n",
    "    image_uri=config[\"image\"],\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    py_version=config[\"py_version\"],\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the Processing Step that will run the evaluation script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "evaluate_model_step = ProcessingStep(\n",
    "    name=\"evaluate-model\",\n",
    "    step_args=evaluation_processor.run(\n",
    "        code=f\"{(CODE_FOLDER / 'evaluation' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            # The first input is the test split that we generated on\n",
    "            # the first step of the pipeline when we split and\n",
    "            # transformed the data.\n",
    "            ProcessingInput(\n",
    "                source=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                    \"test\"\n",
    "                ].S3Output.S3Uri,\n",
    "                destination=\"/opt/ml/processing/test\",\n",
    "            ),\n",
    "            # The second input is the model that we generated on\n",
    "            # the Training or Tunning Step.\n",
    "            ProcessingInput(\n",
    "                source=model_assets,\n",
    "                destination=\"/opt/ml/processing/model\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            # The output is the evaluation report that we generated\n",
    "            # in the evaluation script.\n",
    "            ProcessingOutput(\n",
    "                output_name=\"evaluation\",\n",
    "                source=\"/opt/ml/processing/evaluation\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Creating the Pipeline\n",
    "We can no create a pipeline and submits its definition to the SageMaker Pipeline Service to create the pipeline if it doesn't exist (or update it if it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.processing:Uploaded None to s3://mlschool-tests/tune-model-step/code/dcb1a1f793193c0e310e6d0f91fd9390/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool-tests/tune-model-step/code/f3b7867d7495763812a03744135acb08/runproc.sh\n"
     ]
    }
   ],
   "source": [
    "from functions import submit_pipeline\n",
    "\n",
    "steps = [\n",
    "            preprocessing_step, # type: ignore\n",
    "            tune_model_step if USE_TUNING_STEP else train_model_step, # type: ignore\n",
    "            evaluate_model_step, # type: ignore\n",
    "        ] \n",
    "pipeline_name = \"tune-model-step\"\n",
    "\n",
    "pipeline = submit_pipeline(dataset_location, pipeline_name, steps, pipeline_definition_config, config, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# | eval: false\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've run that we can log into SageMaker Studio and then visit the Pipelines section to see the pipeline we just created. When we do and we visit the graph of the model we'll see the following (with the evaluation step we just created at the bottom):\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/SageMaker_tuning_step.jpg\" style=\"width: 100%; height: auto;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering the Model \n",
    "\n",
    "We're now going to create a step for our pipeline to [register the model](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-version.html) in the [SageMaker Model Directory](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html).\n",
    "\n",
    "When we evaluated our model in the evaluation step we created some metrics for this step that we can use to assess how good our model is before saving it. The idea is: if a model is good enough then we will register it. We want to attach these metrics as metadata to our model (so that when we look at it in the model directory, we can see how well it performed on the test set as well).\n",
    "\n",
    "The inputs into this step are the model and the metrics that come from the evaluation step. SageMaker will then register our model in the model registry.\n",
    "\n",
    "## Step 1: Configuring the Model Package Group\n",
    "The model directory uses groups to organise versions of our model. Let's give this group a name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_MODEL_PACKAGE_GROUP = \"penguin-models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating the Model\n",
    "\n",
    "The model that we created during the Training Step was made using TensorFlow. SageMaker gives us a class that we can use to represent a TensorFlow model. We need this because pert of the information that is going to be stored in the model directory is the docker container that we need to use to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_assets, # AWS URL that points to the training/tuning step\n",
    "    framework_version=config[\"framework_version\"],\n",
    "    sagemaker_session=config[\"session\"],\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuring Model Metrics\n",
    "\n",
    "Now we need to specify the metrics for our model that we're going to attach to the model before it is saved to the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(\n",
    "            on=\"/\",\n",
    "            values=[\n",
    "                evaluate_model_step.properties.ProcessingOutputConfig.Outputs[ # type: ignore\n",
    "                    \"evaluation\"\n",
    "                ].S3Output.S3Uri,\n",
    "                \"evaluation.json\",\n",
    "            ],\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Registering the Model\n",
    "\n",
    "Now it's time to register the model. To do this we're going to write a function to handle this for us (as we'll be using it again throughout the project). We're going to be using something called [ModelStep](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/sagemaker.workflow.pipelines.html#sagemaker.workflow.model_step.ModelStep) to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.tensorflow.model:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "\n",
    "def create_registration_step(\n",
    "    model,\n",
    "    model_package_group_name,\n",
    "    approval_status=\"Approved\",\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    model_metrics=None,\n",
    "    drift_check_baselines=None,\n",
    "):\n",
    "    \"\"\"Create a Registration Step using the supplied parameters.\"\"\"\n",
    "    return ModelStep(\n",
    "        name=\"register\",\n",
    "        step_args=model.register(\n",
    "            model_package_group_name=model_package_group_name,\n",
    "            approval_status=approval_status,\n",
    "            model_metrics=model_metrics,\n",
    "            drift_check_baselines=drift_check_baselines,\n",
    "            content_types=content_types,\n",
    "            response_types=response_types,\n",
    "            inference_instances=[config[\"instance_type\"]],\n",
    "            transform_instances=[config[\"instance_type\"]],\n",
    "            framework_version=config[\"framework_version\"],\n",
    "            domain=\"MACHINE_LEARNING\",\n",
    "            task=\"CLASSIFICATION\",\n",
    "            framework=\"TENSORFLOW\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "register_model_step = create_registration_step(\n",
    "    tensorflow_model,\n",
    "    BASIC_MODEL_PACKAGE_GROUP,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Creating the Pipeline\n",
    "\n",
    "We can no create a pipeline and submits its definition to the SageMaker Pipeline Service to create the pipeline if it doesn't exist (or update it if it does)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.processing:Uploaded None to s3://mlschool-tests/register-model/code/dcb1a1f793193c0e310e6d0f91fd9390/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool-tests/register-model/code/f3b7867d7495763812a03744135acb08/runproc.sh\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.processing:Uploaded None to s3://mlschool-tests/register-model/code/dcb1a1f793193c0e310e6d0f91fd9390/sourcedir.tar.gz\n",
      "INFO:sagemaker.processing:runproc.sh uploaded to s3://mlschool-tests/register-model/code/f3b7867d7495763812a03744135acb08/runproc.sh\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from functions import submit_pipeline\n",
    "\n",
    "steps = [\n",
    "            preprocessing_step, # type: ignore\n",
    "            tune_model_step if USE_TUNING_STEP else train_model_step, # type: ignore\n",
    "            evaluate_model_step, # type: ignore\n",
    "            register_model_step,\n",
    "        ] \n",
    "pipeline_name = \"register-model\"\n",
    "\n",
    "pipeline = submit_pipeline(dataset_location, pipeline_name, steps, pipeline_definition_config, config, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# | eval: false\n",
    "pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
