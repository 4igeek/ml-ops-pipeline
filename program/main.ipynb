{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-To-End MLOps Pipeline With SageMaker (AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by setting up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import boto3\n",
    "import ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this project we will have the ability to run our functions in local mode or not. By setting this to `True` we will run all of the pipelines locally (and not connect to AWS). Setting it to `False` will run the pipeline in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load the environment variables that we need to run this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# This stops SageMaker from reporting in this cell.\n",
    "logging.getLogger('sagemaker.config').disabled = True\n",
    "\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "import sagemaker\n",
    "\n",
    "CODE_FOLDER = Path(\"code\") \n",
    "CODE_FOLDER.mkdir(exist_ok=True) \n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "DATA_FILEPATH = \"penguins.csv\" # Path to data\n",
    "\n",
    "ipytest.autoconfig(raise_on_error=True) # Testing library\n",
    "\n",
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "COMET_API_KEY = os.environ.get(\"COMET_API_KEY\", None)\n",
    "COMET_PROJECT_NAME = os.environ.get(\"COMET_PROJECT_NAME\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise our pipeline session and initialise required config variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Windows Support for Local Mode is Experimental\n"
     ]
    }
   ],
   "source": [
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else LocalPipelineSession(default_bucket=bucket)\n",
    "instance_type = \"ml.m5.xlarge\" if not LOCAL_MODE else \"local\"\n",
    "\n",
    "config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": instance_type,\n",
    "        \"image\": None,\n",
    "    }\n",
    "\n",
    "config[\"framework_version\"] = \"2.12\"\n",
    "config[\"py_version\"] = \"py310\"\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\"\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting & Transforming The Data\n",
    "Now we're going to start building a pipeline (which is just a chain of components). The first component will be for transforming our data.\n",
    "\n",
    "*Note: If you haven't set up the project yet then you need to read [Configuring AWS & Local Environment for MLOps Pipelines Project](https://digitalredneck.co.uk/configuring-aws-local-environment-for-mlops-pipelines-project/) to get up to this point.* \n",
    "\n",
    "We've already uploaded our data to s3 so what this pipeline component is going to do is go to the s3 bucket and retrieve the data and then create a model out of that dataset. In order to do that there are a number of steps that we need to go through. We need to process the data, then we need to train the model, then we need to evaluate the model to make sure it is producing good results, and then we'll be in the position where we can register the model that we can then deploy for use in the future.\n",
    "\n",
    "The pipeline is the structure that is going to combine all of these steps and execute them one after the other.\n",
    "\n",
    "<img src=\"https://digitalredneck.co.uk/PipeLine_split_transform.jpg\" style=\"width: 100%; height: auto;\" />\n",
    "\n",
    "This step is going to take the data from our s3 bucket and then split the data (training and testing sets) and transform the data (replacing missing values etc). The output of this component is going to be saved back in s3. Then the next step will be training and then evaluation etc.\n",
    "\n",
    "We'll use the [Scikit-Learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for the transformations, and a [Processing Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-processing) with a [SKLearnProcessor](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/sagemaker.sklearn.html#scikit-learn-processor) to execute a processing script. For more information on this check out the [SageMaker Pipelines Overview](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-sdk.html) for an intro into the fundamental components of a SageMaker Pipeline.\n",
    "\n",
    "## Step 1: The Processing Script\n",
    "We need to write a script that will be used to split and transform the data. This Processing Step will create a SageMaker Processing Job in the background. It will then run the script and upload the output to AWS s3.\n",
    "\n",
    "We will create a folder called \"processing\" and add it to the system path so that we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line in the following cell i.e. `%%writefile {CODE_FOLDER}/processing/script.py` essentially writes the code written in the cell to a new file in a folder called `processing` without executing it. This means we're using Jupyter to write python files that will then be used in production (and not Jupyter itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/processing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load, split, and transform the data.\"\"\"\n",
    "\n",
    "    # Get all of the data in our data directory and receive a \n",
    "    # shuffled dataset that contains all of the data.\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    # This is where we create a SciKit-Learn pipeline to transform the dataset.\n",
    "\n",
    "    # Create a ColumnTransformer for the target feature.\n",
    "    # This will transform the categorical data in the target feature\n",
    "    # using an ordinal encoder: so 0, 1, 2 etc for the different classes.\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])],\n",
    "    )\n",
    "\n",
    "    # This is applied to all of the numerical columns in the dataset.\n",
    "    # We're going to impute missing values with the mean of all the other values.\n",
    "    # We're also going to scale (normalise) all of the values.\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "    # This will be applied to all of the categorical columns in the dataset.\n",
    "    # We're going to impute missing values with the most common of all the other values.\n",
    "    # We're also going to one-hot encode the columns to create new features.\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(),\n",
    "    )\n",
    "\n",
    "    # This is where we use the transformers we just created in a ColumnTransformer.\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"numeric\", # The name of this transformer\n",
    "                numeric_transformer,\n",
    "                make_column_selector(dtype_exclude=\"object\"),\n",
    "            ),\n",
    "            (\n",
    "                \"categorical\", # The name of this transformer \n",
    "                categorical_transformer, \n",
    "                [\"island\"]), # Only being applied to the island column.\n",
    "                # We don't transform the \"sex\" column and as such SciKit Learn \n",
    "                # will drop it. By ignoring it it will be dropped.\n",
    "                # The reason we're doing that is because \"sex\" doesn't have any \n",
    "                # predictive power in this dataset and as such we don't need it.\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Now that the data has been transformed we're now going to split it.\n",
    "    # We need to split the data before transforming it.\n",
    "    # 70% for training, 15% for validation, and 15% for test\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    # This is going to help us when we get to monitoring our model\n",
    "    # These are the baselines for our raw data.\n",
    "    _save_train_baseline(base_directory, df_train)\n",
    "    _save_test_baseline(base_directory, df_test)\n",
    "\n",
    "    # This is where we now transform the data. This is how the transformers are applied.\n",
    "    # This will be applied to all of the sets in our data i.e. train, validation, and test.\n",
    "    # We \"fit\" the train set and from the info collected from that we will then transform\n",
    "    # The validation and test set using the info collected by calling \"fit_transform\".\n",
    "    \n",
    "    # The reason we do this is in case some of the classes are missing in the validation \n",
    "    # or test sets. If they are we will know because we got that from \"fit_transform\"\n",
    "    # By doing it this way, we can ensure that the encodings of the data are the same \n",
    "    # in all sets i.e. class A=0, class B=1, class C=2 etc. \n",
    "    y_train = target_transformer.fit_transform( # fit_transform \n",
    "        np.array(df_train.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_validation = target_transformer.transform( # transform\n",
    "        np.array(df_validation.species.values).reshape(-1, 1),\n",
    "    )\n",
    "    y_test = target_transformer.transform( # transform\n",
    "        np.array(df_test.species.values).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "    # Now that we've got our \"y\" labels we can drop them from the three main datasets.\n",
    "    # We are going to drop the target feature for the three sets.\n",
    "    df_train = df_train.drop(\"species\", axis=1)\n",
    "    df_validation = df_validation.drop(\"species\", axis=1)\n",
    "    df_test = df_test.drop(\"species\", axis=1)\n",
    "\n",
    "    # This is where we transform the features in the dataset (minus the target column).\n",
    "    X_train = features_transformer.fit_transform(df_train)  # noqa: N806\n",
    "    X_validation = features_transformer.transform(df_validation)  # noqa: N806\n",
    "    X_test = features_transformer.transform(df_test)  # noqa: N806\n",
    "\n",
    "    # Once this is done our data is ready to be saved.\n",
    "    _save_splits(\n",
    "        base_directory,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        X_test,\n",
    "        y_test,\n",
    "    )\n",
    "\n",
    "    # This is where we save the SciKit Learn transformation pipeline.\n",
    "    # When we deploy this model in production we are going to be receiving raw data.\n",
    "    # Before we can make a prediction on that raw data we need to transform the data\n",
    "    # so that it works with the model (i.e. has had the same transformations made to it).\n",
    "    # We need to transform that raw data using the exact same process as we have here.\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # This script will grab data out of every csv file in the directory\n",
    "    # This is because more data can be added to the project (in separate files)\n",
    "    # And that new data will be included in the processing step.\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    # If there are no files then we are going to raise an error\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    # If there are files then we are going to save them to a DataFrame\n",
    "    # This DataFrame will contain the data from all of the files in the directory.\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle and return the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _split_data(df):\n",
    "    \"\"\"Split the data into train, validation, and test.\"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "\n",
    "def _save_train_baseline(base_directory, df_train):\n",
    "    \"\"\"Save the untransformed training data to disk.\n",
    "\n",
    "    We will need the training data to compute a baseline to\n",
    "    determine the quality of the data that the model receives\n",
    "    when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"train-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_train.copy().dropna()\n",
    "\n",
    "    # To compute the data quality baseline, we don't need the\n",
    "    # target variable, so we'll drop it from the dataframe.\n",
    "    df = df.drop(\"species\", axis=1)\n",
    "\n",
    "    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "def _save_test_baseline(base_directory, df_test):\n",
    "    \"\"\"Save the untransformed test data to disk.\n",
    "\n",
    "    We will need the test data to compute a baseline to\n",
    "    determine the quality of the model predictions when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"test-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_test.copy().dropna()\n",
    "\n",
    "    # We'll use the test baseline to generate predictions later,\n",
    "    # and we can't have a header line because the model won't be\n",
    "    # able to make a prediction for it.\n",
    "    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,  # noqa: N803\n",
    "    y_train,\n",
    "    X_validation,  # noqa: N803\n",
    "    y_validation,\n",
    "    X_test,  # noqa: N803\n",
    "    y_test,\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        validation_path / \"validation.csv\",\n",
    "        header=False,\n",
    "        index=False,\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "def _save_model(base_directory, target_transformer, features_transformer):\n",
    "    \"\"\"Save the Scikit-Learn transformation pipelines.\n",
    "\n",
    "    This function creates a model.tar.gz file that\n",
    "    contains the two transformation pipelines we built\n",
    "    to transform the data.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n",
    "        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n",
    "            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n",
    "            tar.add(\n",
    "                Path(directory) / \"features.joblib\", arcname=\"features.joblib\",\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the script to ensure everything worked okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.40s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "# | code-fold: true\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tempfile\n",
    "\n",
    "import pytest\n",
    "from processing.script import preprocess\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@pytest.fixture(autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "\n",
    "    directory = Path(directory)\n",
    "    preprocess(base_directory=directory)\n",
    "\n",
    "    yield directory\n",
    "\n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_preprocess_generates_data_splits(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train\" in output_directories\n",
    "    assert \"validation\" in output_directories\n",
    "    assert \"test\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_generates_baselines(directory):\n",
    "    output_directories = os.listdir(directory)\n",
    "\n",
    "    assert \"train-baseline\" in output_directories\n",
    "    assert \"test-baseline\" in output_directories\n",
    "\n",
    "\n",
    "def test_preprocess_creates_two_models(directory):\n",
    "    model_path = directory / \"model\"\n",
    "    tar = tarfile.open(model_path / \"model.tar.gz\", \"r:gz\")\n",
    "\n",
    "    assert \"features.joblib\" in tar.getnames()\n",
    "    assert \"target.joblib\" in tar.getnames()\n",
    "\n",
    "\n",
    "def test_splits_are_transformed(directory):\n",
    "    train = pd.read_csv(directory / \"train\" / \"train.csv\", header=None)\n",
    "    validation = pd.read_csv(directory / \"validation\" / \"validation.csv\", header=None)\n",
    "    test = pd.read_csv(directory / \"test\" / \"test.csv\", header=None)\n",
    "\n",
    "    # After transforming the data, the number of features should be 7:\n",
    "    # * 3 - island (one-hot encoded)\n",
    "    # * 1 - culmen_length_mm = 1\n",
    "    # * 1 - culmen_depth_mm\n",
    "    # * 1 - flipper_length_mm\n",
    "    # * 1 - body_mass_g\n",
    "    number_of_features = 7\n",
    "\n",
    "    # The transformed splits should have an additional column for the target\n",
    "    # variable.\n",
    "    assert train.shape[1] == number_of_features + 1\n",
    "    assert validation.shape[1] == number_of_features + 1\n",
    "    assert test.shape[1] == number_of_features + 1\n",
    "\n",
    "\n",
    "def test_train_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"train-baseline\" / \"train-baseline.csv\",\n",
    "        header=None,\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 0].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_test_baseline_is_not_transformed(directory):\n",
    "    baseline = pd.read_csv(\n",
    "        directory / \"test-baseline\" / \"test-baseline.csv\", header=None\n",
    "    )\n",
    "\n",
    "    island = baseline.iloc[:, 1].unique()\n",
    "\n",
    "    assert \"Biscoe\" in island\n",
    "    assert \"Torgersen\" in island\n",
    "    assert \"Dream\" in island\n",
    "\n",
    "\n",
    "def test_train_baseline_includes_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"train-baseline\" / \"train-baseline.csv\")\n",
    "    assert baseline.columns[0] == \"island\"\n",
    "\n",
    "\n",
    "def test_test_baseline_does_not_include_header(directory):\n",
    "    baseline = pd.read_csv(directory / \"test-baseline\" / \"test-baseline.csv\")\n",
    "    assert baseline.columns[0] != \"island\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Caching Config\n",
    "SageMaker supports caching which means it will try to execute a previous run of the same step (if nothing has changed). For more info check out [Caching Pipeline](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html).\n",
    "\n",
    "Let's define a caching policy that we can use at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Pipeline Config\n",
    "We can make our pipeline more flexible by parameterising it. In our case we are going to pass through the location of the dataset. This means we can then switch out datasets by changing the value of this parameter. For more information visit [Pipeline Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-parameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setting up the Processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    # By default, a new account doesn't have access to `ml.m5.xlarge` instances.\n",
    "    # If you haven't requested a quota increase yet, you can use an\n",
    "    # `ml.t3.medium` instance type instead. This will work out of the box, but\n",
    "    # the Processing Job will take significantly longer than it should have.\n",
    "    # To get access to `ml.m5.xlarge` instances, you can request a quota\n",
    "    # increase under the Service Quotas section in your AWS account.\n",
    "    instance_type=config[\"instance_type\"],\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=config[\"session\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the Processing Step that we'll use in our pipeline.\n",
    "\n",
    "We specify the list of inputs. In this instance it is the dataset that we stored in s3 (make sure you've completed that by visiting [Configuring AWS & Local Environment for MLOps Pipelines Project](https://digitalredneck.co.uk/configuring-aws-local-environment-for-mlops-pipelines-project/) to ensure you're properly set up to run this notebook project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(CODE_FOLDER / 'processing' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"validation\",\n",
    "                source=\"/opt/ml/processing/validation\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/validation\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"model\",\n",
    "                source=\"/opt/ml/processing/model\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/model\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train-baseline\",\n",
    "                source=\"/opt/ml/processing/train-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train-baseline\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test-baseline\",\n",
    "                source=\"/opt/ml/processing/test-baseline\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test-baseline\",\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "Now we're going to extend the pipeline that we just created with a [Training Step](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html#step-type-training). We're going to be using TensorFlow for that so if you need to know more about that then visit the [TensorFlow docs in SageMaker](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#train-a-model-with-tensorflow).\n",
    "\n",
    "We're also going to implement tracking using [https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html](Amazon SageMaker Experiments) and [Comet ML](https://www.comet.com/site/).\n",
    "\n",
    "To do this we're going to create a new folder in the project called `training` and this will house the script ()which we can then use later by importing it as a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"training\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating the Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/training/script.py\n",
    "# | filename: script.py\n",
    "# | code-line-numbers: true\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "from pathlib import Path\n",
    "from comet_ml import Experiment\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import Input\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from packaging import version\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(\n",
    "    model_directory,\n",
    "    train_path,\n",
    "    validation_path,\n",
    "    pipeline_path,\n",
    "    experiment,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "):\n",
    "    print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_validation = pd.read_csv(Path(validation_path) / \"validation.csv\")\n",
    "    y_validation = X_validation[X_validation.columns[-1]]\n",
    "    X_validation = X_validation.drop(X_validation.columns[-1], axis=1)\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            Input(shape=(X_train.shape[1],)),\n",
    "            Dense(10, activation=\"relu\"),\n",
    "            Dense(8, activation=\"relu\"),\n",
    "            Dense(3, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=SGD(learning_rate=0.01),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        validation_data=(X_validation, y_validation),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=2,\n",
    "    )\n",
    "\n",
    "    predictions = np.argmax(model.predict(X_validation), axis=-1)\n",
    "    val_accuracy = accuracy_score(y_validation, predictions)\n",
    "    print(f\"Validation accuracy: {val_accuracy}\")\n",
    "\n",
    "    # Starting on version 3, Keras changed the model saving format.\n",
    "    # Since we are running the training script using two different versions\n",
    "    # of Keras, we need to check to see which version we are using and save\n",
    "    # the model accordingly.\n",
    "    model_filepath = (\n",
    "        Path(model_directory) / \"001\"\n",
    "        if version.parse(keras.__version__) < version.parse(\"3\")\n",
    "        else Path(model_directory) / \"penguins.keras\"\n",
    "    )\n",
    "\n",
    "    model.save(model_filepath)\n",
    "\n",
    "    # Let's save the transformation pipelines inside the\n",
    "    # model directory so they get bundled together.\n",
    "    with tarfile.open(Path(pipeline_path) / \"model.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(model_directory)\n",
    "\n",
    "    if experiment:\n",
    "        experiment.log_parameters(\n",
    "            {\n",
    "                \"epochs\": epochs,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        )\n",
    "        experiment.log_dataset_hash(X_train)\n",
    "        experiment.log_confusion_matrix(\n",
    "            y_validation.astype(int), predictions.astype(int)\n",
    "        )\n",
    "        experiment.log_model(\"penguins\", model_filepath.as_posix())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Any hyperparameters provided by the training job are passed to\n",
    "    # the entry point as script arguments.\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Let's create a Comet experiment to log the metrics and parameters\n",
    "    # of this training job.\n",
    "    comet_api_key = os.environ.get(\"COMET_API_KEY\", None)\n",
    "    comet_project_name = os.environ.get(\"COMET_PROJECT_NAME\", None)\n",
    "\n",
    "    experiment = (\n",
    "        Experiment(\n",
    "            project_name=comet_project_name,\n",
    "            api_key=comet_api_key,\n",
    "            auto_metric_logging=True,\n",
    "            auto_param_logging=True,\n",
    "            log_code=True,\n",
    "        )\n",
    "        if comet_api_key and comet_project_name\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    training_env = json.loads(os.environ.get(\"SM_TRAINING_ENV\", {}))\n",
    "    job_name = training_env.get(\"job_name\", None) if training_env else None\n",
    "\n",
    "    # We want to use the SageMaker's training job name as the name\n",
    "    # of the experiment so we can easily recognize it.\n",
    "    if job_name and experiment:\n",
    "        experiment.set_name(job_name)\n",
    "\n",
    "    train(\n",
    "        # This is the location where we need to save our model.\n",
    "        # SageMaker will create a model.tar.gz file with anything\n",
    "        # inside this directory when the training script finishes.\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        # SageMaker creates one channel for each one of the inputs\n",
    "        # to the Training Step.\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_VALIDATION\"],\n",
    "        pipeline_path=os.environ[\"SM_CHANNEL_PIPELINE\"],\n",
    "        experiment=experiment,\n",
    "        epochs=args.epochs,\n",
    "        batch_size=args.batch_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.14.0\n",
      "8/8 - 0s - loss: 1.1912 - accuracy: 0.4226 - val_loss: 1.2092 - val_accuracy: 0.2941 - 445ms/epoch - 56ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Validation accuracy: 0.29411764705882354\n",
      "\u001b[32m.\u001b[0mKeras version: 2.14.0\n",
      "8/8 - 1s - loss: 1.1659 - accuracy: 0.3473 - val_loss: 1.0872 - val_accuracy: 0.4314 - 519ms/epoch - 65ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "Validation accuracy: 0.43137254901960786\n",
      "\u001b[32m.\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.36s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -s\n",
    "#| code-fold: true\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pytest\n",
    "import tempfile\n",
    "\n",
    "from processing.script import preprocess\n",
    "from training.script import train\n",
    "\n",
    "@pytest.fixture(scope=\"function\", autouse=False)\n",
    "def directory():\n",
    "    directory = tempfile.mkdtemp()\n",
    "    input_directory = Path(directory) / \"input\"\n",
    "    input_directory.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(DATA_FILEPATH, input_directory / \"data.csv\")\n",
    "    \n",
    "    directory = Path(directory)\n",
    "    \n",
    "    preprocess(base_directory=directory)\n",
    "    train(\n",
    "        model_directory=directory / \"model\",\n",
    "        train_path=directory / \"train\", \n",
    "        validation_path=directory / \"validation\",\n",
    "        pipeline_path=directory / \"model\",\n",
    "        experiment=None,\n",
    "        epochs=1\n",
    "    )\n",
    "    \n",
    "    yield directory\n",
    "    \n",
    "    shutil.rmtree(directory)\n",
    "\n",
    "\n",
    "def test_train_bundles_model_assets(directory):\n",
    "    bundle = os.listdir(directory / \"model\")\n",
    "    assert \"001\" in bundle\n",
    "    \n",
    "    assets = os.listdir(directory / \"model\" / \"001\")\n",
    "    assert \"saved_model.pb\" in assets\n",
    "\n",
    "\n",
    "def test_train_bundles_transformation_pipelines(directory):\n",
    "    bundle = os.listdir(directory / \"model\")\n",
    "    assert \"target.joblib\" in bundle\n",
    "    assert \"features.joblib\" in bundle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Creating the Training Step\n",
    "Now we can create the Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/training/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/training/requirements.txt\n",
    "#| label: requirements.txt\n",
    "#| filename: requirements.txt\n",
    "#| code-line-numbers: false\n",
    "\n",
    "comet_ml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
